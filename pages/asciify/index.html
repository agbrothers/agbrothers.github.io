<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Associative Memory | Greyson Brothers</title>
  <!-- IMPORT TAILWIND -->
  <script src="https://cdn.tailwindcss.com"></script>
  <!-- LINK STYLES (load after Tailwind so it can override) -->
  <link rel="stylesheet" href="/assets/styles.css" />
  <!-- IMPORT JS -->
  <script type="module" src="/assets/main.js"></script>  
</head>


<body class="bg-gray-50 text-gray-800 scroll-smooth">
  <!-- NAVBAR -->
  <header class="fixed inset-x-0 top-0 z-50 bg-white/80 backdrop-blur shadow-md">
    <nav class="sticky top-0 z-50">
      <div class="mx-auto max-w-5xl px-4 sm:px-6">
        <div class="flex h-14 items-center justify-between">
          <!-- NAME -->
          <a href="/" class="font-semibold tracking-tight">Greyson Brothers</a>

          <!-- LINKS -->
          <div class="hidden sm:flex items-center gap-6">
            <a href="/#home" class="btn-link">Home</a>
            <a href="/#projects" class="btn-link">Projects</a>
            <a href="/#publications" class="btn-link">Publications</a>
          </div>

          <!-- HAMBURGER MENU -->
          <button
            id="nav-toggle"
            class="sm:hidden inline-flex items-center justify-center rounded-md p-2 hover:bg-black/5"
            aria-controls="nav-menu"
            aria-expanded="false"
            aria-label="Open menu"
            type="button"
          >
            <!-- HAMBURGER ICON -->
            <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none"
                viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
              <path stroke-linecap="round" stroke-linejoin="round"
                    d="M4 6h16M4 12h16M4 18h16" />
            </svg>
          </button>
        </div>

        <!-- MOBILE MENU PANEL -->
        <div id="nav-menu" class="sm:hidden hidden pb-3">
          <a href="/" class="block rounded-md px-2 py-2 hover:bg-black/5">Home</a>
          <a href="/#projects" class="block rounded-md px-2 py-2 hover:bg-black/5">Projects</a>
          <a href="/#publications" class="block rounded-md px-2 py-2 hover:bg-black/5">Publications</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="mx-auto max-w-3xl px-4 sm:px-6 lg:px-0 pt-16">
    <!-- HEADER -->
    <h1 class="text-4xl font-extrabold tracking-tight mt-6">
      Image-to-ASCII using Key-Value Memory
    </h1>
    <!-- SUB HEADER --> 
    <div class="mt-4 flex flex-wrap items-baseline gap-x-3 gap-y-2">
        <em class="body-text">A visual guide.</em>
        <a class="btn-link" href="https://github.com/agbrothers/asciify" target="_blank" rel="noopener">GitHub</a>
        <em class="ml-auto text-sm text-gray-500 whitespace-nowrap">Dec 28, 2025.</em>
    </div>
    
    <hr class="w-[100%] my-4 mx-auto border-gray-200">    
    <p align="center">
        <a href="https://github.com/agbrothers/asciify" title="GitHub">
            <span style="display:flex; justify-content:center; gap:16px; align-items:flex-start; flex-wrap:wrap;">
                <img src="content/steve-jobs.png" width="46%" style="max-width:46%; height:auto;" />
                <img src="content/steve-jobs-ascii.png" width="46%" style="max-width:46%; height:auto;" />
            </span>
        </a>
    </p>

    <hr class="w-[100%] my-4 mx-auto border-gray-200">
    <div class="codeblock">
        <button class="copy-btn" type="button" aria-label="Copy code">Copy</button>
        <pre><code>pip install "git+https://github.com/agbrothers/asciify.git"</code></pre>
    </div>

    <!-- HEADER VIDEO -->
    <!-- <div class="mt-8">
      <video
        src="content/surface-video-ascii.mp4"
        class="w-full rounded-lg shadow"
        autoplay
        loop
        muted
        playsinline
        controls
      ></video>
      <p class="mt-2 text-sm text-gray-600">
        Caption / explanation of what the viewer is seeing.
      </p>
    </div> -->


    <!-- BODY -->
    <section class="mt-10">
        <!-- OVERVIEW -->
        <h2 class="text-2xl font-semibold">Overview</h2>
        <p class="mt-3 body-text">
            In an effort to clean up old code, I recently reworked an image-to-ASCII script I made years ago. 
            My goals were to make the code both more effective (better quality conversions) and more efficient 
            (able to convert live video streams). In doing so, I made a number of surprising connections to machine 
            learning concepts I've gained experience with in the years since. Namely, (1) this whole algorithm
            can be mapped onto a broader class of key-value memories <a href="#ref-1">[1]</a>, and (2) the conversion can be sped up
            significantly by using 2D convolutions. In fact, the whole process can be viewed simply as pixel-character
            association in an embedding space. Below is a visual guide through this process. 
        </p><br/>

        <!-- CHARACTER ENCODING -->
        <hr class="w-[95%] my-4 mx-auto border-gray-200">
        <p align="center">
            <img src="content/ascii-kv.svg" width="60%" />
        </p>
        <hr class="w-[95%] my-4 mx-auto border-gray-200">
        <p class="mt-2 text-sm text-gray-500">
            <strong>Figure 1.</strong><em> Building a key-value memory from character embeddings and glyphs.</em> 
        </p><br/>

        <!-- HOW IT WORKS -->
        <h2 class="text-2xl font-semibold">How it works</h2>
        <p class="mt-3 body-text">
            For each patch of pixels in an image, our goal is to retrieve the the ASCII character with the most similar 
            shape and brightness. First, we define a palette: the set of ASCII characters we want to use to paint our 
            picture. We render each character in our palette to obtain a glyph, a 2D array of the character in pixels. 
            These are 15x8 pixels using default size 12 Menlo font. 
            <!-- We store the set of glyphs as the values of our memory, the things we 
            ultimately want to retrieve. -->

            <!-- 1. Render each character as a 2D array called a glyph, ~15x8 pixels with the default font size. 

            2. Apply a hand-crafted 2D convolution to each glyph to pull out a feature vector corresponding to the character's shape and brightness. *NOTE*: We use a kernel with the same dimensions and stride of each character glyph.  

            3. Store the set of convolutional feature vectors as keys and the glyph arrays as values in a hetero-associative memory.  -->
        </p><br/>
        
        <p class="mt-3 body-text">
            Just like a hash table, or dictionary, we want to store our characters as a set of key-value pairs. In this
            case, the values are our glyphs, the items we ultimately want to retrieve. Keys, on the other hand, are usually
            simple descriptors used to access the corresponding value, like an address. For this application, we want our
            keys to be lightweight numerical descriptions of the brightness and shape of each character, so that we can use
            a kind of similarity metric with pixel patches to facilitate fast lookup. 
        </p><br/>
        <p class="mt-3 body-text">
            A well-known and efficient method to extract descriptive features from pixels is the 2D convolution <a href="#ref-2">[2]</a>. 
            This involves taking a set of filters (kernels), each of which computes a weighted average of the character pixels with a goal
            of detecting features, like an edge or object. We design the filters to be the same height and width as our character glyphs so
            that the output of each filter is a single scalar value indicating the presence of the corresponding feature. The weights of 
            these filters are set by hand to detect 34 different features of interest, including horizontal lines, vertical lines, diagonal 
            lines, and circles in different parts of the glyph, as well as overall brightness. We then collect the outputs from all filters 
            into into a single feature vector that compactly describes the character, which we store as a key as shown in Figure 1 above. 
            With this, our ASCII memory is ready for action. 
        </p><br/>


        <!-- IMAGE ENCODING -->
        <hr class="w-[95%] my-4 mx-auto border-gray-200">
        <p align="center">
            <img src="content/ascii-q.svg" width="60%" />
        </p>
        <hr class="w-[95%] my-4 mx-auto border-gray-200">
        <p class="mt-2 text-sm text-gray-500">
            <strong>Figure 2.</strong><em> Generating queries from image patches using 2D convolutions the same shape as our character glyphs.</em> 
        </p><br/>

        <p class="mt-3 body-text">
            Now we need to determine how to compare our input image with our keys. Similar to the way a vision transformer 
            processes images, we will just partition each frame into a set of patches <a href="#ref-3">[3]</a>, then compare each
            pixel patch to the keys of our memory to identify the character with the most similar features. By setting these patches 
            to be the same size as our glyphs, we can conventiently use the same convolutional filters to extract patch features as 
            shown in Figure 2 above, as well as easily swap patches for glyphs as shown in Figure 3 below.  
            
            <!-- The filters, or convolutional kernels, we use are normalized such that the presence of a feature yields an output 
            of 1 and the lack of a feature yields an output of zero. These filters are hand-crafted to extract character-level 
            features from the pixels, including horizontal, vertical, diagonal lines, as well as rings and rounded edges. These 
            are likely similar to what one might find in a convolutional neural network trained on MNIST.  -->

            <!-- The mapping 
            function applies a hand-crafted 2D convolution to transform each patch into 
            a 6-dimensional representation. Ahead of time, we pre-render each character 
            into pixel space, then apply a separate 2D kernel to get 6-dimensional 
            representations for the whole ascii character palette being used. The final 
            step simpling maps each patch to the most similar character in our 
            representation space, using euclidean distance. 

            4. Given an image, partition it into patches the size of our character glyphs. Apply the convolution to each patch to generate a query feature vector.  -->
        </p><br/>

        <!-- CHARACTER RECALL -->
        <hr class="w-[95%] my-4 mx-auto border-gray-200">    
        <p align="center">
            <span style="display:flex; justify-content:center; gap:16px; align-items:flex-start; flex-wrap:wrap;">
                <video 
                    src="content/einstein-ascii.mp4" 
                    width="45%" 
                    style="max-width:45%; height:auto;" 
                    autoplay
                    loop
                    muted
                    playsinline
                    controls                
                ></video>
                <img src="content/ascii-association.svg" width="49%" style="max-width:49%; height:auto;" />
            </span>
        </p>
        <hr class="w-[95%] my-4 mx-auto border-gray-200">
        <p class="mt-2 text-sm text-gray-500">
            <strong>Figure 3.</strong><em> We use nearest-neighbor association between a query and the set of stored keys to recall 
                the most similar character glyph for each pixel patch.</em> 
        </p><br/>

        <p class="mt-3 body-text">
            Finally, the recall process. Here, we simply choose a similarity metric, in our case negative distance, 
            and pick the glyph whose key is most similar to the query. We then swap out the pixels for the glyph and 
            repeat. This can be implemented very efficiently under the hood using pytorch 2D convolutions, tensor indexing,
            and reshaping. 
        </p> 

        

        <!-- ROOM FOR IMPROVEMENT -->
        <br/><h2 class="text-2xl font-semibold">Room for Improvement</h2>

        <p class="mt-3 body-text">
            Since we are using nearest neighbor lookup, our character embedding space resembles a high-dimensional 
            voronoi diagram, as shown in Figure 4 below. Each light blue point corresponds to an embedded character,
            and each character has a basin of attraction around it that "captures" any pixel patches that get mapped
            into it. 
        </p><br/>

        <!-- VORONOI DIAGRAMS -->
        <hr class="w-[99%] my-4 mx-auto border-gray-200">
        <p align="center">
            <span style="display:flex; justify-content:center; gap:16px; align-items:flex-start; flex-wrap:wrap;">
                <img src="content/voronoi.png" width="45%" style="max-width:45%; height:auto;" />
                <img src="content/voronoi-char.png" width="49%" style="max-width:49%; height:auto;" />
            </span>
        </p>        
        <hr class="w-[99%] my-4 mx-auto border-gray-200">
        <p class="mt-2 text-sm text-gray-500">
            <strong>Figure 4.</strong><em> Left, voronoi cells as basins of attraction. Right, a voronoi cells of our actual key embeddings, 
            showing how the distribution is sparse in the darker region near the origin.</em> 
        </p><br/>

        <p class="mt-3 body-text">
            Why is this a limitation? Well, any characters that are at the extremes of the feature space, 
            like very dark, bold characters ("N", "M", "W") or very light characters (".", "â€¢"), naturally have 
            larger basins of attraction, because few competing characters exist to bound their cells. 
            Compounding this issue, even the darkest of characters have a lot of white space, so all of our keys
            are biased towards the lighter region of our space. The images we are drawing queries from, however, are
            much more uniformly distributed in the pixel color space, often having rich details in dark parts of 
            the image. When most of the dark pixel patches fall into the voronoi cells for "M" and "N", the resulting 
            image loses a significant amount of detail as a direct result of lacking competition. As shown in the figure 
            below, maximizing competition in voronoi space is the key to getting good results. 
            
            <!-- if you look at the points towards the edge of the space, you'll notice 
            that they have massive, unbounded basins of attraction. In our space, these correspond to the 
            heavier, darker characters, like "#", "M", "N", and "W". Why is this? Any characters that are at the
            extremes of the feature space, like very dark, bold characters or very light characters, act as the boundary voronoi cells -->
        </p><br/>

        <!-- VORONOI DIAGRAMS -->
        <hr class="w-[99%] my-4 mx-auto border-gray-200">
        <div class="grid grid-cols-2 gap-4 justify-items-center">
            <div class="flex flex-col items-center gap-4 w-full">
                <img src="content/voronoi-no-exp.png" class="w-[90%] h-auto" />
                <img src="content/einstein-ascii-no-exp.png" class="w-[90%] h-auto" />
            </div>
            
            <div class="flex flex-col items-center gap-4 w-full">
                <img src="content/voronoi-exp-0.5.png" class="w-[90%] h-auto" />
                <img src="content/einstein-ascii.png" class="w-[90%] h-auto" />
            </div>
        </div>

        <hr class="w-[99%] my-4 mx-auto border-gray-200">
        <p class="mt-2 text-sm text-gray-500">
            <strong>Figure 5.</strong><em> The effect of exposure on image query embeddings (blue). Left, we plot the distribution of queries from an unedited 
            image over the voronoi plot. Right, we apply an exposure value of 0.5, which brightens all query patches and moves 
            the distribution over a more competitive region of the voronoi space. This recovers a significant amount of detail from the dark portions of image.</em> 
        </p><br/> 

        <p class="mt-3 body-text">
            So, how can we remedy this? 
        </p><br/>
    
        <ol class="list-decimal pl-6 space-y-2 mt-3 body-text">
            <li><strong>Add more characters to the palette.</strong> Specifically, adding a larger diversity of characters near 
                the feature extremes. The ideal palatte should result in as even of a tiling of the embedding space as possible</li>
            <li><strong>Define a better kernel.</strong> Perhaps even separate kernels for embedding pixels and characters. The ideal 
                character kernel should accurately capture a variety of shape features and cleanly disentangle shape from brightness. 
                Perhaps this is an area where learned filters could yield an interesting result.</li>
            <li><strong>Image preprocessing.</strong> Rather than making our character embedding space more uniform with respect to the 
                embedded pixel patches, we can squash our pixel space queries to map more uniformly over our character voronoi cells 
                through methods like increasing exposure or contrast, as shown in the Figure 5 above.</li>
        </ol><br/>

        <p class="mt-3 body-text">
            In the current implementation, (2) is set by hand, while (1) and (3) are left up to user input. There is much room for improvement in all three of areas. 
        </p>

        
        <br/><h2 class="text-2xl font-semibold">Why not use the glyphs themselves as keys?</h2>
        <p class="mt-3 body-text">
            Why bother with convolutions and handcrafted features at all, when we could just flatten the glyphs and use 
            them as both the keys and values? Well, unlike what we just discussed, such a feature space does not disentangle 
            shape features from brightness of characters and pixel queries, and is heavily biased towards the latter. It also  
            seems to excacerbate the problem where a handful of heaver or broader characters have basins of attraction that 
            dominate the space. The result is a memory that is bad at both representing its values and constructing its queries, 
            resulting in both poor competition and poor pixel-to-character shape association. Thus a large amount of detail in the 
            resulting image is washed away, as shown in the figure below. 
        </p>
        <!-- FEATURE-LEVEL DIAGRAMS -->
        <hr class="w-[99%] my-4 mx-auto border-gray-200">
        <p align="center">
            <span style="display:flex; justify-content:center; gap:16px; align-items:flex-start; flex-wrap:wrap;">
                <img src="content/einstein-ascii-kv.png" width="47%" style="max-width:47%; height:auto;" />
                <img src="content/einstein-ascii.png" width="47%" style="max-width:47%; height:auto;" />
            </span>
        </p>        
        <hr class="w-[99%] my-4 mx-auto border-gray-200">
        <p class="mt-2 text-sm text-gray-500">
            <strong>Figure 5.</strong><em> Left, using the flattened glyphs as the keys and flattened pixel patches as the queries. 
            Right, using the hand-crafted convolutional filters proposed in the previous sections. The same parameters and image
            processing was applied to both instances.</em> 
        </p><br/> 

        <br/><h2 class="text-2xl font-semibold">Connections to Machine Learning</h2>
        <p class="mt-3 body-text">
            Beyond the surface-level nearest neighbor interpretation, it can be interesting to connect this simple process to
            other relevant algorithms in machine learning. Anyone who has studied transformers probably jumped to connect
            the query-key-value concepts here to attention. Indeed, this can be viewed as a form of hard cross-attention, replacing 
            the dot product association with negative distance for computing attention weights. To make the attention "hard", the 
            softmax is replaced by an argmax over the keys <a href="#ref-4">[4]</a><a href="#ref-5">[5]</a>. Unsurprisingly, the ascii 
            memory association process can also be likened to the update step for recall in a generalized dense associative memory. 
            As in our attention caveat, our similarity function here would be negative distance and we could achieve approximately 
            "hard" memory associations by setting the temperature of our energy function to be near zero <a href="#ref-6">[6]</a>. 
            Most glaringly, though, our whole memory inference pipeline looks remarkably similar to the design of a VQ-VAE 
            <a href="#ref-7">[7]</a>, an earlier generative model designed and trained self-supervizedly for image synthesis. 
            The VQ-VAE encodes an image through convolution, then takes the latent encodings for overlapping patches of the image, 
            looks up the most similar item from a learned dictionary of embeddings, and swaps each patch encoding for a dictionary 
            embedding before passing this latent representation to a decoder. Under this lens, one might view the VQ-VAE instead 
            as a crude form of autoassociative memory. 
        </p> 
        <p class="mt-3 body-text">
            We might also relate this approach to an older line of work: self-organizing maps <a href="#ref-8">[8]</a>. Kohonen 
            developed these simple neural network models in the 80's as a method for learning structured representations from
            data in an unsupervised manner. They feature a set of neurons which compete to fire for a given input. Only the neuron 
            that activates strongest for a given input will fire, hence the names winner-takes-all activation. Then winner then 
            updates itself and its neighbors to be more similar to the input, with this process known as competitive learning. 
            Ignoring the topological aspects, we can view our character key-value memory as loosely similar to the inference 
            loop of a self-organizing map, with our keys corresponding to the neurons in the map, and the winning neuron 
            corresponding our most similar key. More interestingly, self-organizing maps encounter many of the same difficulties 
            we discussed above, namely regarding how initialization of neurons influences their resulting competition over inputs. 
            Choosing a bad set of initial neurons might result in a handful of them always winning, hoarding all of the updates and
            learning signal. As in other aspects of life, monopolies induce bad dynamics. The stifling of competition prevents any 
            of the other neurons from learning diverse and meaningful representations, resulting in a rather useless model of the data. 
            Similarly, when we chose a poor embedding method for our keys in the last section, a small handful of characters 
            monopolized our embedding space, resulting in an ASCII image with little to no detail. 
        </p>
        <p class="mt-3 body-text">
            For a project that looks like pure image processing on the surface, there are a surprising number deep ties
            to ideas and problems in machine learning. My goal here was both provide a visually intuitive primer on key-value 
            memory and to demonstrate that there may be many more algorithms that fall under this general category of memory than 
            we realize. 
        </p>
            <!-- RBF Networks
            VQ-VAE and diffusion models -->


            <!-- Kohonen, Teuvo (1982). "Self-Organized Formation of Topologically Correct Feature Maps". Biological Cybernetics. -->

            <!-- This same process can be viewed under a number of different lenses: nearest neighbor search, hard attention <a href="#ref-4">[4]</a>, 
            associative memory recall with low temperature <a href="#ref-N">[?]</N>, or even a winner-takes-all activation in a self-organizing map <a href="#ref-N">[?]</N>. 

            In fact, this whole process, from start to finish, looks eerily similar to the design of a VQ-VAE <a href="#ref-5">[5]</a>, an earlier generative model designed 
            and trained self-supervizedly for image synthesis. 

            All of these are different ways of describing a very simple process: choose a distance metric and pick the closest thing. 

            5. Compute the similarity between the queries and keys in the ASCII memory (negative distance by default), then retrieve the most similar glyph. 

            6. Insert the retrieved glyphs where each query patch used to be, yielding the converted image.          -->
        </p>
        
        <!-- <p class="mt-3 body-text">
            The image partitioning and embedding process almost exactly mirrors what is done in vision transformers. 

            The process of looking up embeddings from the input patches is highly similar to the core of a VQ-VAE. 

            In this implementation, we use hard, one-step association rather than computing smooth steps over time 
            until convergence. This is because we care more about (1) efficiency, (2) always retrieving an exact 
            "memory", or glyph. With more general associative memory formulations with soft association, we can end 
            up retrieving a spurious combination of multiple memories if their keys are close together in the 
            embedding space.  
        </p> -->



        <!-- REFERENCES -->
        <br/><h2 id="refs" class="text-2xl font-semibold">References</h2>

        <ol class="mt-4 list-decimal pl-6 space-y-2 body-text">
            
            <li id="ref-1" class="ref-item scroll-mt-24">
                <a href="https://arxiv.org/pdf/2501.02950">
                    <!-- Author. <em>Title</em>. Venue, Year. -->
                    S. Gershman, I. Fiete, K. Irie 
                    <em>Key-value memory in the brain</em>. 
                    Neuron, 2025.
                </a>
            </li>
            <li id="ref-2" class="ref-item scroll-mt-24">
                <a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">
                    <!-- Author. <em>Title</em>. Venue, Year. -->
                    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 
                    <em>Gradient Based Learning Applied to Document Recognition</em>. 
                    Proceedings of the IEEE, 1998.
                </a>
            </li>
            <li id="ref-3" class="ref-item scroll-mt-24">
                <a href="https://arxiv.org/pdf/2010.11929">
                    <!-- Author. <em>Title</em>. Venue, Year. -->
                    A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby.
                    <em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em>. 
                    International Conference on Learning Representations, 2021. 
                </a>
            </li>
            <li id="ref-4" class="ref-item scroll-mt-24">
                <a href="https://arxiv.org/pdf/1502.03044">
                    <!-- Author. <em>Title</em>. Venue, Year. -->
                    K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel and Y. Bengio
                    <em>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</em>. 
                    Proceedings of the 32nd International Conference on Machine Learning, 2015. 
                </a>
            </li>
            <li id="ref-5" class="ref-item scroll-mt-24">
                <a href="https://arxiv.org/pdf/1508.04025">
                    <!-- Author. <em>Title</em>. Venue, Year. -->
                    T. Luong, H. Pham, and C.D. Manning,
                    <em>Effective Approaches to Attention-based Neural Machine Translation</em>. 
                    Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015. 
                </a>
            </li>
            <li id="ref-6" class="ref-item scroll-mt-24">
                <a href="https://arxiv.org/pdf/2507.06211">
                    <!-- Author. <em>Title</em>. Venue, Year. -->
                    D. Krotov, B. Hoover, P. Ram, and B. Pham. 
                    <em>Modern Methods in Associative Memory</em>. 
                    International Conference on Machine Learning, 2025. 
                </a>
            </li>
            <li id="ref-7" class="ref-item scroll-mt-24">
                <a href="https://arxiv.org/pdf/1711.00937">
                    <!-- Author. <em>Title</em>. Venue, Year. -->
                    A. Oord, O. Vinyals, and K. Kavukcuoglu.
                    <em>Neural Discrete Representation Learning</em>. 
                    Neural Information Processing Systems, 2017. 
                </a>
            </li>
            <li id="ref-8" class="ref-item scroll-mt-24">
                <a href="https://link.springer.com/article/10.1007/BF00337288">
                    <!-- Author. <em>Title</em>. Venue, Year. -->
                    T. Kohonen.
                    <em>Self-organized formation of topologically correct feature maps</em>. 
                    Biological Cybernetics, 1982. 
                </a>
            </li>
        </ol>

    </section>
  </main>
</body>
</html>
